{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shruti-2608/starting-/blob/main/Brain_tumor_mine.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip freeze\n",
        "! pip uninstall -r pkg.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f9MkJtsQ_9Hy",
        "outputId": "f611b3f3-1b2c-41b0-ae6a-8275b4c68d06"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "absl-py==1.4.0\n",
            "aiohttp==3.8.4\n",
            "aiosignal==1.3.1\n",
            "alabaster==0.7.13\n",
            "albumentations==1.2.1\n",
            "altair==4.2.2\n",
            "anyio==3.7.1\n",
            "appdirs==1.4.4\n",
            "argon2-cffi==21.3.0\n",
            "argon2-cffi-bindings==21.2.0\n",
            "array-record==0.4.0\n",
            "arviz==0.15.1\n",
            "astropy==5.2.2\n",
            "astunparse==1.6.3\n",
            "async-timeout==4.0.2\n",
            "attrs==23.1.0\n",
            "audioread==3.0.0\n",
            "autograd==1.6.2\n",
            "Babel==2.12.1\n",
            "backcall==0.2.0\n",
            "beautifulsoup4==4.11.2\n",
            "bleach==6.0.0\n",
            "blis==0.7.9\n",
            "blosc2==2.0.0\n",
            "bokeh==2.4.3\n",
            "branca==0.6.0\n",
            "build==0.10.0\n",
            "CacheControl==0.13.1\n",
            "cachetools==5.3.1\n",
            "catalogue==2.0.8\n",
            "certifi==2023.5.7\n",
            "cffi==1.15.1\n",
            "chardet==4.0.0\n",
            "charset-normalizer==2.0.12\n",
            "chex==0.1.7\n",
            "click==8.1.4\n",
            "click-plugins==1.1.1\n",
            "cligj==0.7.2\n",
            "cloudpickle==2.2.1\n",
            "cmake==3.25.2\n",
            "cmdstanpy==1.1.0\n",
            "colorcet==3.0.1\n",
            "colorlover==0.3.0\n",
            "community==1.0.0b1\n",
            "confection==0.1.0\n",
            "cons==0.4.6\n",
            "contextlib2==21.6.0\n",
            "contourpy==1.1.0\n",
            "convertdate==2.4.0\n",
            "cufflinks==0.17.3\n",
            "cvxopt==1.3.1\n",
            "cvxpy==1.3.2\n",
            "cycler==0.11.0\n",
            "cymem==2.0.7\n",
            "Cython==0.29.36\n",
            "dask==2022.12.1\n",
            "datascience==0.17.6\n",
            "db-dtypes==1.1.1\n",
            "dbus-python==1.2.16\n",
            "debugpy==1.6.6\n",
            "decorator==4.4.2\n",
            "defusedxml==0.7.1\n",
            "distributed==2022.12.1\n",
            "dlib==19.24.2\n",
            "dm-tree==0.1.8\n",
            "docutils==0.16\n",
            "dopamine-rl==4.0.6\n",
            "duckdb==0.8.1\n",
            "earthengine-api==0.1.358\n",
            "easydict==1.10\n",
            "ecos==2.0.12\n",
            "editdistance==0.6.2\n",
            "en-core-web-sm @ https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.5.0/en_core_web_sm-3.5.0-py3-none-any.whl#sha256=0964370218b7e1672a30ac50d72cdc6b16f7c867496f1d60925691188f4d2510\n",
            "entrypoints==0.4\n",
            "ephem==4.1.4\n",
            "et-xmlfile==1.1.0\n",
            "etils==1.3.0\n",
            "etuples==0.3.9\n",
            "exceptiongroup==1.1.2\n",
            "fastai==2.7.12\n",
            "fastcore==1.5.29\n",
            "fastdownload==0.0.7\n",
            "fastjsonschema==2.17.1\n",
            "fastprogress==1.0.3\n",
            "fastrlock==0.8.1\n",
            "filelock==3.12.2\n",
            "Fiona==1.9.4.post1\n",
            "firebase-admin==5.3.0\n",
            "Flask==2.2.5\n",
            "flatbuffers==23.5.26\n",
            "flax==0.7.0\n",
            "folium==0.14.0\n",
            "fonttools==4.40.0\n",
            "frozendict==2.3.8\n",
            "frozenlist==1.3.3\n",
            "fsspec==2023.6.0\n",
            "future==0.18.3\n",
            "gast==0.4.0\n",
            "gcsfs==2023.6.0\n",
            "GDAL==3.3.2\n",
            "gdown==4.6.6\n",
            "gensim==4.3.1\n",
            "geographiclib==2.0\n",
            "geopandas==0.13.2\n",
            "geopy==2.3.0\n",
            "gin-config==0.5.0\n",
            "glob2==0.7\n",
            "google==2.0.3\n",
            "google-api-core==2.11.1\n",
            "google-api-python-client==2.84.0\n",
            "google-auth==2.17.3\n",
            "google-auth-httplib2==0.1.0\n",
            "google-auth-oauthlib==1.0.0\n",
            "google-cloud-bigquery==3.10.0\n",
            "google-cloud-bigquery-connection==1.12.1\n",
            "google-cloud-bigquery-storage==2.22.0\n",
            "google-cloud-core==2.3.3\n",
            "google-cloud-datastore==2.15.2\n",
            "google-cloud-firestore==2.11.1\n",
            "google-cloud-functions==1.13.1\n",
            "google-cloud-language==2.9.1\n",
            "google-cloud-storage==2.8.0\n",
            "google-cloud-translate==3.11.2\n",
            "google-colab @ file:///colabtools/dist/google-colab-1.0.0.tar.gz#sha256=70f4248ae4c75dcfdcc7d95dae63c3e0ac697149dc4febc5ae9a1a168e025236\n",
            "google-crc32c==1.5.0\n",
            "google-pasta==0.2.0\n",
            "google-resumable-media==2.5.0\n",
            "googleapis-common-protos==1.59.1\n",
            "googledrivedownloader==0.4\n",
            "graphviz==0.20.1\n",
            "greenlet==2.0.2\n",
            "grpc-google-iam-v1==0.12.6\n",
            "grpcio==1.56.0\n",
            "grpcio-status==1.48.2\n",
            "gspread==3.4.2\n",
            "gspread-dataframe==3.3.1\n",
            "gym==0.25.2\n",
            "gym-notices==0.0.8\n",
            "h5netcdf==1.2.0\n",
            "h5py==3.8.0\n",
            "holidays==0.28\n",
            "holoviews==1.15.4\n",
            "html5lib==1.1\n",
            "httpimport==1.3.0\n",
            "httplib2==0.21.0\n",
            "humanize==4.6.0\n",
            "hyperopt==0.2.7\n",
            "idna==3.4\n",
            "imageio==2.25.1\n",
            "imageio-ffmpeg==0.4.8\n",
            "imagesize==1.4.1\n",
            "imbalanced-learn==0.10.1\n",
            "imgaug==0.4.0\n",
            "importlib-resources==6.0.0\n",
            "imutils==0.5.4\n",
            "inflect==6.0.5\n",
            "iniconfig==2.0.0\n",
            "intel-openmp==2023.1.0\n",
            "ipykernel==5.5.6\n",
            "ipython==7.34.0\n",
            "ipython-genutils==0.2.0\n",
            "ipython-sql==0.4.1\n",
            "ipywidgets==7.7.1\n",
            "itsdangerous==2.1.2\n",
            "jax==0.4.13\n",
            "jaxlib @ https://storage.googleapis.com/jax-releases/cuda11/jaxlib-0.4.13+cuda11.cudnn86-cp310-cp310-manylinux2014_x86_64.whl#sha256=af30095a0adf342b837a0ed9607e13177ee66f4e654c031a383aa546cd21d815\n",
            "jieba==0.42.1\n",
            "Jinja2==3.1.2\n",
            "joblib==1.3.1\n",
            "jsonpickle==3.0.1\n",
            "jsonschema==4.3.3\n",
            "jupyter-client==6.1.12\n",
            "jupyter-console==6.1.0\n",
            "jupyter-server==1.24.0\n",
            "jupyter_core==5.3.1\n",
            "jupyterlab-pygments==0.2.2\n",
            "jupyterlab-widgets==3.0.8\n",
            "kaggle==1.5.15\n",
            "keras==2.12.0\n",
            "kiwisolver==1.4.4\n",
            "langcodes==3.3.0\n",
            "lazy_loader==0.3\n",
            "libclang==16.0.0\n",
            "librosa==0.10.0.post2\n",
            "lightgbm==3.3.5\n",
            "lit==16.0.6\n",
            "llvmlite==0.39.1\n",
            "locket==1.0.0\n",
            "logical-unification==0.4.6\n",
            "LunarCalendar==0.0.9\n",
            "lxml==4.9.3\n",
            "Markdown==3.4.3\n",
            "markdown-it-py==3.0.0\n",
            "MarkupSafe==2.1.3\n",
            "matplotlib==3.7.1\n",
            "matplotlib-inline==0.1.6\n",
            "matplotlib-venn==0.11.9\n",
            "mdurl==0.1.2\n",
            "miniKanren==1.0.3\n",
            "missingno==0.5.2\n",
            "mistune==0.8.4\n",
            "mizani==0.8.1\n",
            "mkl==2019.0\n",
            "ml-dtypes==0.2.0\n",
            "mlxtend==0.22.0\n",
            "more-itertools==9.1.0\n",
            "moviepy==1.0.3\n",
            "mpmath==1.3.0\n",
            "msgpack==1.0.5\n",
            "multidict==6.0.4\n",
            "multipledispatch==1.0.0\n",
            "multitasking==0.0.11\n",
            "murmurhash==1.0.9\n",
            "music21==8.1.0\n",
            "natsort==8.3.1\n",
            "nbclient==0.8.0\n",
            "nbconvert==6.5.4\n",
            "nbformat==5.9.1\n",
            "nest-asyncio==1.5.6\n",
            "networkx==3.1\n",
            "nibabel==3.0.2\n",
            "nltk==3.8.1\n",
            "notebook==6.4.8\n",
            "numba==0.56.4\n",
            "numexpr==2.8.4\n",
            "numpy==1.22.4\n",
            "oauth2client==4.1.3\n",
            "oauthlib==3.2.2\n",
            "opencv-contrib-python==4.7.0.72\n",
            "opencv-python==4.7.0.72\n",
            "opencv-python-headless==4.8.0.74\n",
            "openpyxl==3.0.10\n",
            "opt-einsum==3.3.0\n",
            "optax==0.1.5\n",
            "orbax-checkpoint==0.2.7\n",
            "osqp==0.6.2.post8\n",
            "packaging==23.1\n",
            "palettable==3.3.3\n",
            "pandas==1.5.3\n",
            "pandas-datareader==0.10.0\n",
            "pandas-gbq==0.17.9\n",
            "pandocfilters==1.5.0\n",
            "panel==0.14.4\n",
            "param==1.13.0\n",
            "parso==0.8.3\n",
            "partd==1.4.0\n",
            "pathlib==1.0.1\n",
            "pathy==0.10.2\n",
            "patsy==0.5.3\n",
            "pexpect==4.8.0\n",
            "pickleshare==0.7.5\n",
            "Pillow==8.4.0\n",
            "pip-tools==6.13.0\n",
            "platformdirs==3.8.1\n",
            "plotly==5.13.1\n",
            "plotnine==0.10.1\n",
            "pluggy==1.2.0\n",
            "polars==0.17.3\n",
            "pooch==1.6.0\n",
            "portpicker==1.5.2\n",
            "prefetch-generator==1.0.3\n",
            "preshed==3.0.8\n",
            "prettytable==0.7.2\n",
            "proglog==0.1.10\n",
            "progressbar2==4.2.0\n",
            "prometheus-client==0.17.1\n",
            "promise==2.3\n",
            "prompt-toolkit==3.0.39\n",
            "prophet==1.1.4\n",
            "proto-plus==1.22.3\n",
            "protobuf==3.20.3\n",
            "psutil==5.9.5\n",
            "psycopg2==2.9.6\n",
            "ptyprocess==0.7.0\n",
            "py-cpuinfo==9.0.0\n",
            "py4j==0.10.9.7\n",
            "pyarrow==9.0.0\n",
            "pyasn1==0.5.0\n",
            "pyasn1-modules==0.3.0\n",
            "pycocotools==2.0.6\n",
            "pycparser==2.21\n",
            "pyct==0.5.0\n",
            "pydantic==1.10.11\n",
            "pydata-google-auth==1.8.1\n",
            "pydot==1.4.2\n",
            "pydot-ng==2.0.0\n",
            "pydotplus==2.0.2\n",
            "PyDrive==1.3.1\n",
            "pyerfa==2.0.0.3\n",
            "pygame==2.5.0\n",
            "Pygments==2.14.0\n",
            "PyGObject==3.36.0\n",
            "pymc==5.1.2\n",
            "PyMeeus==0.5.12\n",
            "pymystem3==0.2.0\n",
            "PyOpenGL==3.1.7\n",
            "pyparsing==3.1.0\n",
            "pyproj==3.6.0\n",
            "pyproject_hooks==1.0.0\n",
            "pyrsistent==0.19.3\n",
            "PySocks==1.7.1\n",
            "pytensor==2.10.1\n",
            "pytest==7.2.2\n",
            "python-apt==0.0.0\n",
            "python-dateutil==2.8.2\n",
            "python-louvain==0.16\n",
            "python-slugify==8.0.1\n",
            "python-utils==3.7.0\n",
            "pytz==2022.7.1\n",
            "pyviz-comms==2.3.2\n",
            "PyWavelets==1.4.1\n",
            "PyYAML==6.0\n",
            "pyzmq==23.2.1\n",
            "qdldl==0.1.7.post0\n",
            "qudida==0.0.4\n",
            "regex==2022.10.31\n",
            "requests==2.27.1\n",
            "requests-oauthlib==1.3.1\n",
            "requests-unixsocket==0.2.0\n",
            "requirements-parser==0.5.0\n",
            "rich==13.4.2\n",
            "rpy2==3.5.5\n",
            "rsa==4.9\n",
            "scikit-image==0.19.3\n",
            "scikit-learn==1.2.2\n",
            "scipy==1.10.1\n",
            "scs==3.2.3\n",
            "seaborn==0.12.2\n",
            "Send2Trash==1.8.2\n",
            "shapely==2.0.1\n",
            "six==1.16.0\n",
            "sklearn-pandas==2.2.0\n",
            "smart-open==6.3.0\n",
            "sniffio==1.3.0\n",
            "snowballstemmer==2.2.0\n",
            "sortedcontainers==2.4.0\n",
            "soundfile==0.12.1\n",
            "soupsieve==2.4.1\n",
            "soxr==0.3.5\n",
            "spacy==3.5.4\n",
            "spacy-legacy==3.0.12\n",
            "spacy-loggers==1.0.4\n",
            "Sphinx==3.5.4\n",
            "sphinxcontrib-applehelp==1.0.4\n",
            "sphinxcontrib-devhelp==1.0.2\n",
            "sphinxcontrib-htmlhelp==2.0.1\n",
            "sphinxcontrib-jsmath==1.0.1\n",
            "sphinxcontrib-qthelp==1.0.3\n",
            "sphinxcontrib-serializinghtml==1.1.5\n",
            "SQLAlchemy==2.0.18\n",
            "sqlparse==0.4.4\n",
            "srsly==2.4.6\n",
            "statsmodels==0.13.5\n",
            "sympy==1.11.1\n",
            "tables==3.8.0\n",
            "tabulate==0.8.10\n",
            "tblib==2.0.0\n",
            "tenacity==8.2.2\n",
            "tensorboard==2.12.3\n",
            "tensorboard-data-server==0.7.1\n",
            "tensorflow==2.12.0\n",
            "tensorflow-datasets==4.9.2\n",
            "tensorflow-estimator==2.12.0\n",
            "tensorflow-gcs-config==2.12.0\n",
            "tensorflow-hub==0.13.0\n",
            "tensorflow-io-gcs-filesystem==0.32.0\n",
            "tensorflow-metadata==1.13.1\n",
            "tensorflow-probability==0.20.1\n",
            "tensorstore==0.1.40\n",
            "termcolor==2.3.0\n",
            "terminado==0.17.1\n",
            "text-unidecode==1.3\n",
            "textblob==0.17.1\n",
            "tf-slim==1.1.0\n",
            "thinc==8.1.10\n",
            "threadpoolctl==3.1.0\n",
            "tifffile==2023.7.10\n",
            "tinycss2==1.2.1\n",
            "toml==0.10.2\n",
            "tomli==2.0.1\n",
            "toolz==0.12.0\n",
            "torch @ https://download.pytorch.org/whl/cu118/torch-2.0.1%2Bcu118-cp310-cp310-linux_x86_64.whl#sha256=a7a49d459bf4862f64f7bc1a68beccf8881c2fa9f3e0569608e16ba6f85ebf7b\n",
            "torchaudio @ https://download.pytorch.org/whl/cu118/torchaudio-2.0.2%2Bcu118-cp310-cp310-linux_x86_64.whl#sha256=26692645ea061a005c57ec581a2d0425210ac6ba9f923edf11cc9b0ef3a111e9\n",
            "torchdata==0.6.1\n",
            "torchsummary==1.5.1\n",
            "torchtext==0.15.2\n",
            "torchvision @ https://download.pytorch.org/whl/cu118/torchvision-0.15.2%2Bcu118-cp310-cp310-linux_x86_64.whl#sha256=19ca4ab5d6179bbe53cff79df1a855ee6533c2861ddc7389f68349d8b9f8302a\n",
            "tornado==6.3.1\n",
            "tqdm==4.65.0\n",
            "traitlets==5.7.1\n",
            "triton==2.0.0\n",
            "tweepy==4.13.0\n",
            "typer==0.9.0\n",
            "types-setuptools==68.0.0.1\n",
            "typing_extensions==4.7.1\n",
            "tzlocal==5.0.1\n",
            "uritemplate==4.1.1\n",
            "urllib3==1.26.16\n",
            "vega-datasets==0.9.0\n",
            "wasabi==1.1.2\n",
            "wcwidth==0.2.6\n",
            "webcolors==1.13\n",
            "webencodings==0.5.1\n",
            "websocket-client==1.6.1\n",
            "Werkzeug==2.3.6\n",
            "widgetsnbextension==3.6.4\n",
            "wordcloud==1.8.2.2\n",
            "wrapt==1.14.1\n",
            "xarray==2022.12.0\n",
            "xarray-einstats==0.6.0\n",
            "xgboost==1.7.6\n",
            "xlrd==2.0.1\n",
            "yarl==1.9.2\n",
            "yellowbrick==1.5\n",
            "yfinance==0.2.22\n",
            "zict==3.0.0\n",
            "zipp==3.16.0\n",
            "\u001b[31mERROR: Could not open requirements file: [Errno 2] No such file or directory: 'pkg.txt'\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "caVkjDbnBHFX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f85d37e0-ee50-4bfd-d1e7-58b5bc2679e3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting asm\n",
            "  Downloading asm-0.0.5-py3-none-any.whl (20 kB)\n",
            "Collecting genshi (from asm)\n",
            "  Downloading Genshi-0.7.7-py3-none-any.whl (177 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/177.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.4/177.4 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from genshi->asm) (1.16.0)\n",
            "Installing collected packages: genshi, asm\n",
            "Successfully installed asm-0.0.5 genshi-0.7.7\n"
          ]
        }
      ],
      "source": [
        "!pip install asm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "seqT5o2nBt4p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "244376e0-6dbf-4d90-b402-0bc2960c1cbf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pennylane\n",
            "  Downloading PennyLane-0.31.0-py3-none-any.whl (1.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m30.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy<1.24 in /usr/local/lib/python3.10/dist-packages (from pennylane) (1.22.4)\n",
            "Collecting scipy<=1.10 (from pennylane)\n",
            "  Downloading scipy-1.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.4/34.4 MB\u001b[0m \u001b[31m42.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from pennylane) (3.1)\n",
            "Collecting rustworkx (from pennylane)\n",
            "  Downloading rustworkx-0.13.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m81.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting autograd<=1.5 (from pennylane)\n",
            "  Downloading autograd-1.5-py3-none-any.whl (48 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.9/48.9 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: toml in /usr/local/lib/python3.10/dist-packages (from pennylane) (0.10.2)\n",
            "Requirement already satisfied: appdirs in /usr/local/lib/python3.10/dist-packages (from pennylane) (1.4.4)\n",
            "Collecting semantic-version>=2.7 (from pennylane)\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Collecting autoray>=0.3.1 (from pennylane)\n",
            "  Downloading autoray-0.6.3-py3-none-any.whl (48 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.3/48.3 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: cachetools in /usr/local/lib/python3.10/dist-packages (from pennylane) (5.3.1)\n",
            "Collecting pennylane-lightning>=0.31 (from pennylane)\n",
            "  Downloading PennyLane_Lightning-0.31.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.6/16.6 MB\u001b[0m \u001b[31m61.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from pennylane) (2.27.1)\n",
            "Requirement already satisfied: future>=0.15.2 in /usr/local/lib/python3.10/dist-packages (from autograd<=1.5->pennylane) (0.18.3)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->pennylane) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->pennylane) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->pennylane) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->pennylane) (3.4)\n",
            "Installing collected packages: semantic-version, scipy, rustworkx, autoray, autograd, pennylane-lightning, pennylane\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.10.1\n",
            "    Uninstalling scipy-1.10.1:\n",
            "      Successfully uninstalled scipy-1.10.1\n",
            "  Attempting uninstall: autograd\n",
            "    Found existing installation: autograd 1.6.2\n",
            "    Uninstalling autograd-1.6.2:\n",
            "      Successfully uninstalled autograd-1.6.2\n",
            "Successfully installed autograd-1.5 autoray-0.6.3 pennylane-0.31.0 pennylane-lightning-0.31.0 rustworkx-0.13.0 scipy-1.10.0 semantic-version-2.10.0\n",
            "Collecting git+https://github.com/PennyLaneAI/pennylane-qiskit.git\n",
            "  Cloning https://github.com/PennyLaneAI/pennylane-qiskit.git to /tmp/pip-req-build-jevpmubs\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/PennyLaneAI/pennylane-qiskit.git /tmp/pip-req-build-jevpmubs\n",
            "  Resolved https://github.com/PennyLaneAI/pennylane-qiskit.git to commit fe9c2e94ce4be19dff6851de1b2440faef1bf6e8\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting qiskit>=0.32 (from PennyLane-qiskit==0.32.0.dev0)\n",
            "  Downloading qiskit-0.43.2.tar.gz (9.1 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting qiskit-ibm-runtime (from PennyLane-qiskit==0.32.0.dev0)\n",
            "  Downloading qiskit_ibm_runtime-0.11.2-py3-none-any.whl (128 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.0/128.0 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting qiskit-ibm-provider (from PennyLane-qiskit==0.32.0.dev0)\n",
            "  Downloading qiskit_ibm_provider-0.6.1-py3-none-any.whl (233 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.8/233.8 kB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting mthree>=0.17 (from PennyLane-qiskit==0.32.0.dev0)\n",
            "  Downloading mthree-2.5.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m86.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pennylane>=0.30 in /usr/local/lib/python3.10/dist-packages (from PennyLane-qiskit==0.32.0.dev0) (0.31.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from PennyLane-qiskit==0.32.0.dev0) (1.22.4)\n",
            "Requirement already satisfied: networkx>=2.2 in /usr/local/lib/python3.10/dist-packages (from PennyLane-qiskit==0.32.0.dev0) (3.1)\n",
            "Requirement already satisfied: scipy>=1.3 in /usr/local/lib/python3.10/dist-packages (from mthree>=0.17->PennyLane-qiskit==0.32.0.dev0) (1.10.0)\n",
            "Requirement already satisfied: cython>=0.29 in /usr/local/lib/python3.10/dist-packages (from mthree>=0.17->PennyLane-qiskit==0.32.0.dev0) (0.29.36)\n",
            "Collecting qiskit-terra>=0.21 (from mthree>=0.17->PennyLane-qiskit==0.32.0.dev0)\n",
            "  Downloading qiskit_terra-0.24.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.9/5.9 MB\u001b[0m \u001b[31m95.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting qiskit-ibmq-provider>=0.19.2 (from mthree>=0.17->PennyLane-qiskit==0.32.0.dev0)\n",
            "  Downloading qiskit_ibmq_provider-0.20.2-py3-none-any.whl (241 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m241.5/241.5 kB\u001b[0m \u001b[31m21.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from mthree>=0.17->PennyLane-qiskit==0.32.0.dev0) (5.9.5)\n",
            "Collecting orjson>=3.0.0 (from mthree>=0.17->PennyLane-qiskit==0.32.0.dev0)\n",
            "  Downloading orjson-3.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (138 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.7/138.7 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: rustworkx in /usr/local/lib/python3.10/dist-packages (from pennylane>=0.30->PennyLane-qiskit==0.32.0.dev0) (0.13.0)\n",
            "Requirement already satisfied: autograd<=1.5 in /usr/local/lib/python3.10/dist-packages (from pennylane>=0.30->PennyLane-qiskit==0.32.0.dev0) (1.5)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.10/dist-packages (from pennylane>=0.30->PennyLane-qiskit==0.32.0.dev0) (0.10.2)\n",
            "Requirement already satisfied: appdirs in /usr/local/lib/python3.10/dist-packages (from pennylane>=0.30->PennyLane-qiskit==0.32.0.dev0) (1.4.4)\n",
            "Requirement already satisfied: semantic-version>=2.7 in /usr/local/lib/python3.10/dist-packages (from pennylane>=0.30->PennyLane-qiskit==0.32.0.dev0) (2.10.0)\n",
            "Requirement already satisfied: autoray>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from pennylane>=0.30->PennyLane-qiskit==0.32.0.dev0) (0.6.3)\n",
            "Requirement already satisfied: cachetools in /usr/local/lib/python3.10/dist-packages (from pennylane>=0.30->PennyLane-qiskit==0.32.0.dev0) (5.3.1)\n",
            "Requirement already satisfied: pennylane-lightning>=0.31 in /usr/local/lib/python3.10/dist-packages (from pennylane>=0.30->PennyLane-qiskit==0.32.0.dev0) (0.31.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from pennylane>=0.30->PennyLane-qiskit==0.32.0.dev0) (2.27.1)\n",
            "Collecting qiskit-aer==0.12.1 (from qiskit>=0.32->PennyLane-qiskit==0.32.0.dev0)\n",
            "  Downloading qiskit_aer-0.12.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m57.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting requests-ntlm<=1.1.0 (from qiskit-ibmq-provider>=0.19.2->mthree>=0.17->PennyLane-qiskit==0.32.0.dev0)\n",
            "  Downloading requests_ntlm-1.1.0-py2.py3-none-any.whl (5.7 kB)\n",
            "Requirement already satisfied: urllib3>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from qiskit-ibmq-provider>=0.19.2->mthree>=0.17->PennyLane-qiskit==0.32.0.dev0) (1.26.16)\n",
            "Requirement already satisfied: python-dateutil>=2.8.0 in /usr/local/lib/python3.10/dist-packages (from qiskit-ibmq-provider>=0.19.2->mthree>=0.17->PennyLane-qiskit==0.32.0.dev0) (2.8.2)\n",
            "Requirement already satisfied: websocket-client>=1.5.1 in /usr/local/lib/python3.10/dist-packages (from qiskit-ibmq-provider>=0.19.2->mthree>=0.17->PennyLane-qiskit==0.32.0.dev0) (1.6.1)\n",
            "Collecting websockets>=10.0 (from qiskit-ibmq-provider>=0.19.2->mthree>=0.17->PennyLane-qiskit==0.32.0.dev0)\n",
            "  Downloading websockets-11.0.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ply>=3.10 (from qiskit-terra>=0.21->mthree>=0.17->PennyLane-qiskit==0.32.0.dev0)\n",
            "  Downloading ply-3.11-py2.py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.6/49.6 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sympy>=1.3 in /usr/local/lib/python3.10/dist-packages (from qiskit-terra>=0.21->mthree>=0.17->PennyLane-qiskit==0.32.0.dev0) (1.11.1)\n",
            "Collecting dill>=0.3 (from qiskit-terra>=0.21->mthree>=0.17->PennyLane-qiskit==0.32.0.dev0)\n",
            "  Downloading dill-0.3.6-py3-none-any.whl (110 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting stevedore>=3.0.0 (from qiskit-terra>=0.21->mthree>=0.17->PennyLane-qiskit==0.32.0.dev0)\n",
            "  Downloading stevedore-5.1.0-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.6/49.6 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting symengine<0.10,>=0.9 (from qiskit-terra>=0.21->mthree>=0.17->PennyLane-qiskit==0.32.0.dev0)\n",
            "  Downloading symengine-0.9.2-cp310-cp310-manylinux2010_x86_64.whl (37.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m37.5/37.5 MB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=4.3 in /usr/local/lib/python3.10/dist-packages (from qiskit-ibm-provider->PennyLane-qiskit==0.32.0.dev0) (4.7.1)\n",
            "Collecting ibm-platform-services>=0.22.6 (from qiskit-ibm-runtime->PennyLane-qiskit==0.32.0.dev0)\n",
            "  Downloading ibm-platform-services-0.38.0.tar.gz (254 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m254.2/254.2 kB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: future>=0.15.2 in /usr/local/lib/python3.10/dist-packages (from autograd<=1.5->pennylane>=0.30->PennyLane-qiskit==0.32.0.dev0) (0.18.3)\n",
            "Collecting requests (from pennylane>=0.30->PennyLane-qiskit==0.32.0.dev0)\n",
            "  Downloading requests-2.31.0-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.6/62.6 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ibm-cloud-sdk-core<4.0.0,>=3.16.7 (from ibm-platform-services>=0.22.6->qiskit-ibm-runtime->PennyLane-qiskit==0.32.0.dev0)\n",
            "  Downloading ibm-cloud-sdk-core-3.16.7.tar.gz (52 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.3/52.3 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.0->qiskit-ibmq-provider>=0.19.2->mthree>=0.17->PennyLane-qiskit==0.32.0.dev0) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->pennylane>=0.30->PennyLane-qiskit==0.32.0.dev0) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->pennylane>=0.30->PennyLane-qiskit==0.32.0.dev0) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->pennylane>=0.30->PennyLane-qiskit==0.32.0.dev0) (2023.5.7)\n",
            "Collecting ntlm-auth>=1.0.2 (from requests-ntlm<=1.1.0->qiskit-ibmq-provider>=0.19.2->mthree>=0.17->PennyLane-qiskit==0.32.0.dev0)\n",
            "  Downloading ntlm_auth-1.5.0-py2.py3-none-any.whl (29 kB)\n",
            "Collecting cryptography>=1.3 (from requests-ntlm<=1.1.0->qiskit-ibmq-provider>=0.19.2->mthree>=0.17->PennyLane-qiskit==0.32.0.dev0)\n",
            "  Downloading cryptography-41.0.2-cp37-abi3-manylinux_2_28_x86_64.whl (4.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.3/4.3 MB\u001b[0m \u001b[31m68.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=1.3->requests-ntlm<=1.1.0->qiskit-ibmq-provider>=0.19.2->mthree>=0.17->PennyLane-qiskit==0.32.0.dev0) (1.15.1)\n",
            "Collecting PyJWT<3.0.0,>=2.4.0 (from ibm-cloud-sdk-core<4.0.0,>=3.16.7->ibm-platform-services>=0.22.6->qiskit-ibm-runtime->PennyLane-qiskit==0.32.0.dev0)\n",
            "  Downloading PyJWT-2.7.0-py3-none-any.whl (22 kB)\n",
            "Collecting pbr!=2.1.0,>=2.0.0 (from stevedore>=3.0.0->qiskit-terra>=0.21->mthree>=0.17->PennyLane-qiskit==0.32.0.dev0)\n",
            "  Downloading pbr-5.11.1-py2.py3-none-any.whl (112 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.7/112.7 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy>=1.3->qiskit-terra>=0.21->mthree>=0.17->PennyLane-qiskit==0.32.0.dev0) (1.3.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=1.3->requests-ntlm<=1.1.0->qiskit-ibmq-provider>=0.19.2->mthree>=0.17->PennyLane-qiskit==0.32.0.dev0) (2.21)\n",
            "Building wheels for collected packages: PennyLane-qiskit, qiskit, ibm-platform-services, ibm-cloud-sdk-core\n",
            "  Building wheel for PennyLane-qiskit (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for PennyLane-qiskit: filename=PennyLane_qiskit-0.32.0.dev0-py3-none-any.whl size=28314 sha256=3bc9457a2027f2cd11b5f6b3027935ecfea25c57fd7c8c49de222524ad4b5d43\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-cpths9nj/wheels/5e/c1/f3/811d7c6e97bd9d4a166e8820d0e98ec5bc2e8bad7f4e818ae1\n",
            "  Building wheel for qiskit (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for qiskit: filename=qiskit-0.43.2-py3-none-any.whl size=7640 sha256=723f228e9e243f680495cba69b82eeaa39faf783f586753f8a65a1d94344ddf9\n",
            "  Stored in directory: /root/.cache/pip/wheels/d4/ad/4f/f54eb8743e54c5bab69837b842cf56a8a87ac7c57a2abf85ad\n",
            "  Building wheel for ibm-platform-services (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ibm-platform-services: filename=ibm_platform_services-0.38.0-py3-none-any.whl size=269268 sha256=69c7556cdf18b80848f720fffe75f77a800eb38349297e989b4a8e1cca2d08a4\n",
            "  Stored in directory: /root/.cache/pip/wheels/66/d1/ce/c1e7012107226e244456bb31787947ea489cd6b4ac0dcd4223\n",
            "  Building wheel for ibm-cloud-sdk-core (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ibm-cloud-sdk-core: filename=ibm_cloud_sdk_core-3.16.7-py3-none-any.whl size=85614 sha256=3c1f9118fcec813d2e36c4817d75e337c5f454c4ac89db94039b7d6a0855c746\n",
            "  Stored in directory: /root/.cache/pip/wheels/57/9c/ed/611f998db826deaeedadd824c8a0427a677136325358d79fdc\n",
            "Successfully built PennyLane-qiskit qiskit ibm-platform-services ibm-cloud-sdk-core\n",
            "Installing collected packages: ply, websockets, symengine, requests, PyJWT, pbr, orjson, ntlm-auth, dill, stevedore, ibm-cloud-sdk-core, cryptography, requests-ntlm, qiskit-terra, ibm-platform-services, qiskit-ibmq-provider, qiskit-ibm-provider, qiskit-aer, qiskit-ibm-runtime, qiskit, mthree, PennyLane-qiskit\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.27.1\n",
            "    Uninstalling requests-2.27.1:\n",
            "      Successfully uninstalled requests-2.27.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.27.1, but you have requests 2.31.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed PennyLane-qiskit-0.32.0.dev0 PyJWT-2.7.0 cryptography-41.0.2 dill-0.3.6 ibm-cloud-sdk-core-3.16.7 ibm-platform-services-0.38.0 mthree-2.5.1 ntlm-auth-1.5.0 orjson-3.9.2 pbr-5.11.1 ply-3.11 qiskit-0.43.2 qiskit-aer-0.12.1 qiskit-ibm-provider-0.6.1 qiskit-ibm-runtime-0.11.2 qiskit-ibmq-provider-0.20.2 qiskit-terra-0.24.1 requests-2.31.0 requests-ntlm-1.1.0 stevedore-5.1.0 symengine-0.9.2 websockets-11.0.3\n"
          ]
        }
      ],
      "source": [
        "!pip install pennylane\n",
        "#!pip install pennylane-qiskit\n",
        "!pip install git+https://github.com/PennyLaneAI/pennylane-qiskit.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "IlqYtPloB2hi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f3897e8d-acad-4b48-94ad-8ddd5f6a5097"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.0.1+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.7.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (16.0.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.15.2+cu118)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.22.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision) (2.31.0)\n",
            "Requirement already satisfied: torch==2.0.1 in /usr/local/lib/python3.10/dist-packages (from torchvision) (2.0.1+cu118)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (8.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchvision) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchvision) (4.7.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchvision) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchvision) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchvision) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchvision) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.1->torchvision) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.1->torchvision) (16.0.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2023.5.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.0.1->torchvision) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.0.1->torchvision) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "# PyTorch\n",
        "!pip install torch\n",
        "!pip install torchvision"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F2Re2wIYB_Qn"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2i7LnszqCRZp"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "os.listdir()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bwr1j_LICXwa"
      },
      "outputs": [],
      "source": [
        "for dirname, _, filenames in os.walk('/content/drive/MyDrive/Brain Tumor dataset'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YWNU3hGBFODJ"
      },
      "outputs": [],
      "source": [
        "# System\n",
        "import time\n",
        "import os, platform, sys\n",
        "import copy\n",
        "from datetime import datetime\n",
        "\n",
        "# PyTorch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "# Pennylane\n",
        "import pennylane as qml\n",
        "from pennylane import numpy as np\n",
        "\n",
        "# Plotting\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# OpenMP: number of parallel threads.\n",
        "os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
        "\n",
        "# ibm quantum computer\n",
        "import qiskit\n",
        "from qiskit import IBMQ\n",
        "from qiskit.providers.ibmq import least_busy\n",
        "\n",
        "# tools\n",
        "#from asm_tools import myprint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RUsLlDW8FPe7"
      },
      "outputs": [],
      "source": [
        "print('[OS name]:', os.name, ', system:', platform.system(), \\\n",
        "      ', release:', platform.release(), ', system name:', platform.node())\n",
        "print(\"[Python version]:\", sys.version)\n",
        "print(\"[Python version info]: \", sys.version_info)\n",
        "print(\"[Deep Learning framework, Pytorch (Facebook) version]:\", torch.__version__)\n",
        "print(\"[Qiskit (IBM) version]:\", qiskit.__version__)\n",
        "print(\"[Quantum Machine Learning framework (Pennylane) version]:\", qml.__version__)\n",
        "print(\"[Anaconda version]:\")\n",
        "!conda list anaconda"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gw5JCCEDFazW"
      },
      "outputs": [],
      "source": [
        "num_epochs = 40      # Number of training epochs\n",
        "dataset_dir = \"/content/drive/MyDrive/Brain Tumor dataset\"  # quick train in \"_data/faces\", complete training data in \"_data/faces_google\"\n",
        "\n",
        "n_qubits = 10                       # Number of qubits4\n",
        "step = 0.004\n",
        "#step = 0.01                        # Learning rate\n",
        "batch_size = 50                     # Number of samples for each training step32\n",
        "q_depth = 12                        # Depth of the quantum circuit (number of variational layers)\n",
        "gamma_lr_scheduler = 0.0001\n",
        "#gamma_lr_scheduler = 1           # Learning rate reduction applied every 10 epochs.0.1\n",
        "#q_delta = 0.01\n",
        "q_delta = 0.0001                    # Initial spread of random quantum weights0.001\n",
        "rng_seed = 3                        # Seed for random number generator\n",
        "start_time = time.time()            # Start of the computation timer\n",
        "\n",
        "model_fileext = \".pth\"\n",
        "log_fileext = \".log\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QSYHS6XDvouR"
      },
      "outputs": [],
      "source": [
        "!pwd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HqYOf-NXv3u3"
      },
      "outputs": [],
      "source": [
        "def select_qc_backend():\n",
        "    valid_selections = ('1', '2', '3', '5', '9')\n",
        "    prompt = \"Please select source:\\n \\\n",
        "        1: Pennylane.ai Quantum Simulator\\n \\\n",
        "        2: Qiskit Aer, IBM Quantum Simulator (local)\\n \\\n",
        "        3: Qiskit IBMQ:Aer, IBM Quantum Simulator (IBM Quantum Computing Experience)\\n \\\n",
        "        5: Google Cirq:Simulator (local)\\n \\\n",
        "        9: Qiskit IBMQ:Terra, IBM real Quantum Computer (IBM Quantum Computing Experience)\\n\"\n",
        "    selection = input(prompt)\n",
        "    while not(selection in valid_selections):\n",
        "        selection = input(prompt)\n",
        "    return selection\n",
        "\n",
        "# select backend quantum computer\n",
        "qc_backend = int(select_qc_backend())\n",
        "if qc_backend == 1: # pennylane\n",
        "    backend_name = \"simPennylane\"\n",
        "    print(\"=> Using Pennylane Quantum Computer Simulator (local)\")\n",
        "    dev = qml.device(\"default.qubit\", wires=n_qubits)\n",
        "elif qc_backend == 2: # ibm quantum simulator (local)\n",
        "    backend_name = \"simIBMQLocal\"\n",
        "    print(\"=> Using IBM Quantum Computer Simulator (local)\")\n",
        "    dev = qml.device(\"qiskit.aer\", wires=n_qubits)\n",
        "elif qc_backend == 3: # ibm quantum simulator (cloud)\n",
        "    backend_name = \"simIBMQCloud\"\n",
        "    print(\"=> Using IBM Quantum Computer Simulator (IBM Quantum Computing Experience on IBM Cloud)\")\n",
        "    print(\"Loading IBMQ credentials...\")\n",
        "    IBMQ.load_account()\n",
        "    provider = IBMQ.get_provider('ibm-q')\n",
        "    backend = 'ibmq_qasm_simulator'\n",
        "    dev = qml.device(\"qiskit.ibmq\", wires=n_qubits, backend=backend)\n",
        "    #dev.capabilities()['backend']\n",
        "elif qc_backend == 5: # google quantum simulator (local)\n",
        "    backend_name = \"simGoogleLocal\"\n",
        "    print(\"=> Using Google Quantum Computer Simulator (local)\")\n",
        "    dev = qml.device(\"cirq.simulator\", wires=n_qubits)\n",
        "elif qc_backend == 9: # ibm real quantum computer (cloud)\n",
        "    backend_name = \"realIBMQCloud\"\n",
        "    print(\"=> Using real IBM Quantum Computer (IBM Quantum Computing Experience on IBM Cloud)\")\n",
        "    print(\"Loading IBMQ credentials...\")\n",
        "    IBMQ.load_account()\n",
        "    provider = IBMQ.get_provider('ibm-q')\n",
        "    print(\"Searching available least busy real IBM Quantum Computer...\")\n",
        "    backend = least_busy(provider.backends(filters=lambda x: x.configuration().n_qubits >= 4 \\\n",
        "        and not x.configuration().simulator \\\n",
        "        and x.status().operational==True))\n",
        "    print('using least busy backend:', backend)\n",
        "    dev = qml.device(\"qiskit.ibmq\", wires=n_qubits, backend=str(backend))\n",
        "    #dev.capabilities()['backend']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j4iDaM6bwtyh"
      },
      "outputs": [],
      "source": [
        "now = datetime.now()\n",
        "now_str = now.strftime(\"%d%m%Y%H%M%S\")\n",
        "\n",
        "# model & log file name to generate\n",
        "# swgCQ_resnet18_ + selected backend name + max epochs + current date & time + extension\n",
        "#   - pytorch model file extension '.pth'\n",
        "#   - log file for the generated model file extension '.log'\n",
        "base_filename = \"swgCQ_\"\n",
        "interim_model_name = base_filename + backend_name\n",
        "hybrid_model_name = base_filename + backend_name + \"(\" + str(num_epochs) + \")-\" + now_str + model_fileext\n",
        "log_filename = base_filename + backend_name + \"(\" + str(num_epochs) + \")-\" + now_str + log_fileext\n",
        "\n",
        "train_val_filename = base_filename + backend_name + \"_train_val-results\" + now_str\n",
        "\n",
        "message = \"[Quantum Machine Learning for Image Classification]\\n\" \\\n",
        "    + \"(Transfer Learning: base model is a pretrained resnet-18 (classical), then retrain the fc-layer (quantum) only)\\n\" \\\n",
        "    + \"- @Andi Sama, 2020\\n\" \\\n",
        "    + \"- Sinergi Wahana Gemilang\\n\" \\\n",
        "    + \"- Tools: Python, QML:PennyLane.ai, Quantum Simulator & HW: IBM Q on Cloud, ML-Framework: Pytorch\\n\" \\\n",
        "    + \"- Creating log file:\" + now.strftime(\"%d/%m/%Y %H:%M:%S\") + \"\\n\" \\\n",
        "    + \"- Target model name: \" + hybrid_model_name + \"\\n\" \\\n",
        "    + \"- Selected quantum device backend: \" + str(dev) + \"\\n\"\n",
        "#myprint(log_filename, \"new\", message, screen=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TqmMKc0Uw7wr"
      },
      "outputs": [],
      "source": [
        " #USE NVidia CUDA (GPU) if available\n",
        "# ----------------------------------\n",
        "is_cuda_available = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda:0\" if is_cuda_available else \"cpu\")\n",
        "if is_cuda_available:\n",
        "    print (\"cuda is available, using:\", device)\n",
        "else:\n",
        "    print (\"cuda is not available, using:\", device)\n",
        "\n",
        "message = \"* Processing device: \" + str(device)\n",
        "#myprint(log_filename, \"append\", message, screen=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IkTynDoOYFmW"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pPoaKwIdPsUD"
      },
      "outputs": [],
      "source": [
        "# initialize data loaders\n",
        "data_transforms = {\n",
        "    \"train\": transforms.Compose(\n",
        "        [\n",
        "            # transforms.RandomResizedCrop(224),     # uncomment for data augmentation\n",
        "            # transforms.RandomHorizontalFlip(),     # uncomment for data augmentation\n",
        "            transforms.Resize(256),\n",
        "            transforms.CenterCrop(224),\n",
        "            transforms.ToTensor(),\n",
        "            # Normalize input channels using mean values and standard deviations of ImageNet.\n",
        "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
        "        ]\n",
        "    ),\n",
        "    \"val\": transforms.Compose(\n",
        "        [\n",
        "            transforms.Resize(256),\n",
        "            transforms.CenterCrop(224),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
        "        ]\n",
        "    ),\n",
        "}\n",
        "\n",
        "data_dir = dataset_dir # Images data, faces with and without mask\n",
        "image_datasets = {\n",
        "    x if x == \"train\" else \"validation\": datasets.ImageFolder(os.path.join(data_dir, x), data_transforms[x])\n",
        "    for x in [\"train\", \"val\"]\n",
        "}\n",
        "dataset_sizes = {x: len(image_datasets[x]) for x in [\"train\", \"validation\"]}\n",
        "class_names = image_datasets[\"train\"].classes\n",
        "\n",
        "# Initialize dataloader\n",
        "dataloaders = {\n",
        "    x: torch.utils.data.DataLoader(image_datasets[x], batch_size=batch_size, shuffle=True)\n",
        "    for x in [\"train\", \"validation\"]\n",
        "}\n",
        "\n",
        "# function to plot images\n",
        "def imshow(inp, title=None):\n",
        "    \"\"\"Display image from tensor.\"\"\"\n",
        "    inp = inp.numpy().transpose((1, 2, 0))\n",
        "    # Inverse of the initial normalization operation.\n",
        "    mean = np.array([0.485, 0.456, 0.406])\n",
        "    std = np.array([0.229, 0.224, 0.225])\n",
        "    inp = std * inp + mean\n",
        "    inp = np.clip(inp, 0, 1)\n",
        "    plt.imshow(inp)\n",
        "    if title is not None:\n",
        "        plt.title(title)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-LxluIWwa-Xy"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6KLdbi4iPtsc"
      },
      "outputs": [],
      "source": [
        "# Get a batch of training data\n",
        "inputs, classes = next(iter(dataloaders[\"validation\"]))\n",
        "\n",
        "# Make a grid from batch\n",
        "out = torchvision.utils.make_grid(inputs)\n",
        "\n",
        "imshow(out, title=[class_names[x] for x in classes])\n",
        "\n",
        "# In order to get reproducible results, we set a manual seed for the\n",
        "# random number generator and re-initialize the dataloaders.\n",
        "\n",
        "torch.manual_seed(rng_seed)\n",
        "dataloaders = {\n",
        "    x: torch.utils.data.DataLoader(image_datasets[x], batch_size=batch_size, shuffle=True)\n",
        "    for x in [\"train\", \"validation\"]\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LRRaMDSybL3h"
      },
      "outputs": [],
      "source": [
        "#%cd /content/drive/MyDrive/Brain tumor dataset/train_saved"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fQehoaecbU7Z"
      },
      "outputs": [],
      "source": [
        "# 1st - Prepare the Quantum Gates\n",
        "def H_layer(nqubits):\n",
        "    \"\"\"Layer of single-qubit Hadamard gates.\n",
        "    \"\"\"\n",
        "    for idx in range(nqubits):\n",
        "        qml.Hadamard(wires=idx)\n",
        "\n",
        "def RY_layer(w):\n",
        "    \"\"\"Layer of parametrized qubit rotations around the y axis.\n",
        "    \"\"\"\n",
        "    for idx, element in enumerate(w):\n",
        "        qml.RY(element, wires=idx)\n",
        "\n",
        "def entangling_layer(nqubits):\n",
        "    \"\"\"Layer of CNOTs followed by another shifted layer of CNOT.\n",
        "    \"\"\"\n",
        "    # In other words it should apply something like :\n",
        "    # CNOT  CNOT  CNOT  CNOT...  CNOT\n",
        "    #   CNOT  CNOT  CNOT...  CNOT\n",
        "    for i in range(0, nqubits - 1, 2):  # Loop over even indices: i=0,2,...N-2\n",
        "        qml.CNOT(wires=[i, i + 1])\n",
        "    for i in range(1, nqubits - 1, 2):  # Loop over odd indices:  i=1,3,...N-3\n",
        "        qml.CNOT(wires=[i, i + 1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j_67OOc8p_tS"
      },
      "outputs": [],
      "source": [
        "@qml.qnode(dev, interface=\"torch\")\n",
        "def quantum_net(q_input_features, q_weights_flat):\n",
        "    \"\"\"\n",
        "    The variational quantum circuit.\n",
        "    \"\"\"\n",
        "\n",
        "    # Reshape weights\n",
        "    q_weights = q_weights_flat.reshape(q_depth, n_qubits)\n",
        "\n",
        "    # Start from state |+> , unbiased w.r.t. |0> and |1>\n",
        "    H_layer(n_qubits)\n",
        "\n",
        "    # Embed features in the quantum node\n",
        "    RY_layer(q_input_features)\n",
        "\n",
        "    # Sequence of trainable variational layers\n",
        "    for k in range(q_depth):\n",
        "        entangling_layer(n_qubits)\n",
        "        RY_layer(q_weights[k])\n",
        "\n",
        "    # Expectation values in the Z basis\n",
        "    exp_vals = [qml.expval(qml.PauliZ(position)) for position in range(n_qubits)]\n",
        "    return tuple(exp_vals)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lb9tjaiucKa3"
      },
      "outputs": [],
      "source": [
        "!pwd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ltkPh-yvcON1"
      },
      "outputs": [],
      "source": [
        "# 2nd - Prepare the Replacement Quantum Layer\n",
        "class DressedQuantumNet(nn.Module):\n",
        "    \"\"\"\n",
        "    Torch module implementing the *dressed* quantum net.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        Definition of the *dressed* layout.\n",
        "        \"\"\"\n",
        "\n",
        "        super().__init__()\n",
        "        self.pre_net = nn.Linear(512,n_qubits)\n",
        "        self.q_params = nn.Parameter(q_delta * torch.randn(q_depth * n_qubits))\n",
        "        self.post_net = nn.Linear(n_qubits, 4)\n",
        "\n",
        "    def forward(self, input_features):\n",
        "        \"\"\"\n",
        "        Defining how tensors are supposed to move through the *dressed* quantum\n",
        "        net.\n",
        "        \"\"\"\n",
        "\n",
        "        # obtain the input features for the quantum circuit\n",
        "        # by reducing the feature dimension from 512 to 4\n",
        "        pre_out = self.pre_net(input_features)\n",
        "        q_in = torch.tanh(pre_out) * np.pi / 2.0\n",
        "\n",
        "        # Apply the quantum circuit to each element of the batch and append to q_out\n",
        "        q_out = torch.Tensor(0, n_qubits)\n",
        "        q_out = q_out.to(device)\n",
        "        for elem in q_in:\n",
        "            q_out_elem = torch.tensor(quantum_net(elem, self.q_params), dtype=torch.float32).unsqueeze(0)\n",
        "            q_out_elem = q_out_elem.to(device)  # Move to the same device as q_out\n",
        "            q_out = torch.cat((q_out, q_out_elem))\n",
        "\n",
        "\n",
        "        # return the two-dimensional prediction from the postprocessing layer\n",
        "        return self.post_net(q_out)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uKo-F4tmcaS4"
      },
      "outputs": [],
      "source": [
        "# 3rd - Replace last layer of resnet-50 with defined quantum layer\n",
        "message = \"* Loading pre-trained resnet-18...\"\n",
        "#myprint(log_filename, \"append\", message, screen=True)\n",
        "model_hybrid = torchvision.models.resnet18(pretrained=True)\n",
        "\n",
        "for param in model_hybrid.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Notice that model_hybrid.fc is the last layer of ResNet18\n",
        "message = \"  - Replacing last layer (fc-layer) with Quantum Layer...\"\n",
        "#myprint(log_filename, \"append\", message, screen=True)\n",
        "model_hybrid.fc = DressedQuantumNet()\n",
        "\n",
        "# Use CUDA or CPU according to the \"device\" object.\n",
        "model_hybrid = model_hybrid.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aIBRMQmbciJE"
      },
      "outputs": [],
      "source": [
        "criterion= nn.CrossEntropyLoss()\n",
        "optimizer_hybrid = optim.Adam(model_hybrid.fc.parameters(), lr=step)\n",
        "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_hybrid, step_size=10, gamma=gamma_lr_scheduler)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "84nem9ypc5GE"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F1PF3dYdgU53"
      },
      "source": [
        "## **new one**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DOotgzLRhaD3"
      },
      "outputs": [],
      "source": [
        "def train_model(model, criterion, optimizer, scheduler, num_epochs, temp_model_name):\n",
        "    since = time.time()\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_acc = 0.0\n",
        "    best_loss = 10000.0  # Large arbitrary number\n",
        "    best_acc_train = 0.0\n",
        "    best_loss_train = 10000.0  # Large arbitrary number\n",
        "    message = \"  => Training Started.\"\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        # Each epoch has a training and validation phase\n",
        "        for phase in [\"train\", \"val\"]:\n",
        "            if phase == \"train\":\n",
        "                model.train()\n",
        "            else:\n",
        "                model.eval()\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "\n",
        "            for inputs, labels in dataloaders[phase]:\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                with torch.set_grad_enabled(phase == \"train\"):\n",
        "                    # Forward pass through the quantum layer\n",
        "                    inputs_quantum = model_hybrid.fc.pre_net(inputs)\n",
        "                    inputs_quantum = torch.tanh(inputs_quantum) * np.pi / 2.0\n",
        "                    outputs_quantum = model_hybrid.fc.quantum_net(inputs_quantum, model_hybrid.fc.q_params)\n",
        "                    outputs = model_hybrid.fc.post_net(outputs_quantum)\n",
        "\n",
        "                    _, preds = torch.max(outputs, 1)\n",
        "                    loss = criterion(outputs, labels)\n",
        "\n",
        "                    if phase == \"train\":\n",
        "                        # Backpropagation and optimization\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                # Statistics\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "            # Calculate epoch loss and accuracy\n",
        "            epoch_loss = running_loss / dataset_sizes[phase]\n",
        "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
        "\n",
        "            # Print epoch results\n",
        "            message = \"     > Phase: {} Epoch: {}/{} Loss: {:.4f} Acc: {:.4f}        \".format(\n",
        "                phase,\n",
        "                epoch + 1,\n",
        "                num_epochs,\n",
        "                epoch_loss,\n",
        "                epoch_acc,\n",
        "            )\n",
        "            print(message)\n",
        "\n",
        "            # Check if this is the best model wrt previous epochs\n",
        "            if phase == \"val\" and epoch_acc > best_acc:\n",
        "                best_acc = epoch_acc\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "            if phase == \"val\" and epoch_loss < best_loss:\n",
        "                best_loss = epoch_loss\n",
        "            if phase == \"train\" and epoch_acc > best_acc_train:\n",
        "                best_acc_train = epoch_acc\n",
        "            if phase == \"train\" and epoch_loss < best_loss_train:\n",
        "                best_loss_train = epoch_loss\n",
        "\n",
        "            train_Acc = \"{:.4f}\".format(best_acc_train)\n",
        "            train_Loss = \"{:.4f}\".format(best_loss_train)\n",
        "            val_Acc = \"{:.4f}\".format(best_acc)\n",
        "            val_Loss = \"{:.4f}\".format(best_loss)\n",
        "\n",
        "            # Update learning rate\n",
        "            if phase == \"train\":\n",
        "                scheduler.step()\n",
        "\n",
        "        # Save the retrained model at this epoch completion\n",
        "        model_at_epoch = temp_model_name + \"-at-epoch-\" + str(epoch + 1) + model_fileext\n",
        "        torch.save(model.state_dict(), model_at_epoch)\n",
        "\n",
        "    # Print final results\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    time_elapsed = time.time() - since\n",
        "    total_training_time = \"{:.0f}m {:.0f}s\".format(time_elapsed // 60, time_elapsed % 60)\n",
        "    message = \"  => Training completed in \" + total_training_time\n",
        "    print(message)\n",
        "    message = \"  => Best test loss: {:.4f} | Best test accuracy: {:.4f}\".format(best_loss, best_acc)\n",
        "    print(message)\n",
        "\n",
        "    return model, total_training_time, train_Acc, train_Loss, val_Acc, val_Loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x4ystcywP3rc"
      },
      "outputs": [],
      "source": [
        "def train_model(model, criterion, optimizer, scheduler, num_epochs, temp_model_name):\n",
        "    since = time.time()\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_acc = 0.0\n",
        "    best_loss = 10000.0  # Large arbitrary number\n",
        "    best_acc_train = 0.0\n",
        "    best_loss_train = 10000.0  # Large arbitrary number\n",
        "    message = \"  => Training Started.\"\n",
        "    #myprint(log_filename, \"append\", message, screen=True)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        # Each epoch has a training and validation phase\n",
        "        for phase in [\"train\", \"validation\"]:\n",
        "            if phase == \"train\":\n",
        "                # Set model to training mode\n",
        "                model.train()\n",
        "            else:\n",
        "                # Set model to evaluate mode\n",
        "                model.eval()\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "\n",
        "            # Iterate over data.\n",
        "            n_batches = dataset_sizes[phase] // batch_size\n",
        "            it = 0\n",
        "            for inputs, labels in dataloaders[phase]:\n",
        "                since_batch = time.time()\n",
        "                batch_size_ = len(inputs)\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # Track/compute gradient and make an optimization step only when training\n",
        "                with torch.set_grad_enabled(phase == \"train\"):\n",
        "                    outputs = model(inputs)\n",
        "                    _, preds = torch.max(outputs, 1)\n",
        "                    loss = criterion(outputs, labels)\n",
        "                    if phase == \"train\":\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                # Print iteration results\n",
        "                running_loss += loss.item() * batch_size_\n",
        "                batch_corrects = torch.sum(preds == labels.data).item()\n",
        "                running_corrects += batch_corrects\n",
        "                message = \"     > Phase: {} Epoch: {}/{} Iter: {}/{} Batch time: {:.4f}\".format(\n",
        "                                    phase,\n",
        "                                    epoch + 1,\n",
        "                                    num_epochs,\n",
        "                                    it + 1,\n",
        "                                    n_batches + 1,\n",
        "                                    time.time() - since_batch,\n",
        "                                 )\n",
        "                # Print to file (not to screen)\n",
        "                #myprint(log_filename, \"append\", message, screen=False)\n",
        "                # Print to screen with flush=True\n",
        "                print(message,\n",
        "                        end=\"\\r\",\n",
        "                        flush=True,\n",
        "                )\n",
        "                it += 1\n",
        "\n",
        "            # Print epoch results\n",
        "            epoch_loss = running_loss / dataset_sizes[phase]\n",
        "            epoch_acc = running_corrects / dataset_sizes[phase]\n",
        "            message = \"     > Phase: {} Epoch: {}/{} Loss: {:.4f} Acc: {:.4f}        \".format(\n",
        "                                \"train\" if phase == \"train\" else \"validation  \",\n",
        "                                epoch + 1,\n",
        "                                num_epochs,\n",
        "                                epoch_loss,\n",
        "                                epoch_acc,\n",
        "                            )\n",
        "            #myprint(log_filename, \"append\", message, screen=True)\n",
        "\n",
        "            # Check if this is the best model wrt previous epochs\n",
        "            if phase == \"validation\" and epoch_acc > best_acc:\n",
        "                best_acc = epoch_acc\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "            if phase == \"validation\" and epoch_loss < best_loss:\n",
        "                best_loss = epoch_loss\n",
        "            if phase == \"train\" and epoch_acc > best_acc_train:\n",
        "                best_acc_train = epoch_acc\n",
        "            if phase == \"train\" and epoch_loss < best_loss_train:\n",
        "                best_loss_train = epoch_loss\n",
        "\n",
        "            train_Acc = \"{:.4f}\".format(best_acc_train)\n",
        "            train_Loss = \"{:.4f}\".format(best_loss_train)\n",
        "            val_Acc = \"{:.4f}\".format(best_acc)\n",
        "            val_Loss = \"{:.4f}\".format(best_loss)\n",
        "\n",
        "            # Update learning rate\n",
        "            if phase == \"train\":\n",
        "                scheduler.step()\n",
        "\n",
        "        # save the retrained model at this epoch completion\n",
        "        # epoch is saved as epoch+1 (so to start at 1 instead of 0)\n",
        "        # ---------------------------------------------------------\n",
        "        model_at_epoch = temp_model_name + \"-at-epoch-\"+ str(epoch+1) \\\n",
        "            + \"(\" + str(num_epochs) + \")-\" + now_str + model_fileext\n",
        "        message = \"* Saving interim model while training: \" + model_at_epoch\n",
        "        #myprint(log_filename, \"append\", message, screen=True)\n",
        "        torch.save(model_hybrid.state_dict(), model_at_epoch)\n",
        "\n",
        "        # save results of trained model at this epoch\n",
        "        # -------------------------------------------\n",
        "        # append list of \"epoch, train accuracy, train loss, val accuracy, val loss\" per epoch completion\n",
        "        #   accumulate in train_val_results\n",
        "        if epoch==0:\n",
        "            # create first row\n",
        "            train_val_results = np.array([[epoch+1, best_acc_train, best_loss_train, best_acc, best_loss]])\n",
        "            #myprint(log_filename, \"append\", str(train_val_results), screen=True)\n",
        "        else:\n",
        "            train_result_at_epoch = np.array([[epoch+1, best_acc_train, best_loss_train, best_acc, best_loss]])\n",
        "            # append new row\n",
        "\n",
        "            train_val_results = np.append(train_val_results, train_result_at_epoch, axis=0)\n",
        "            #myprint(log_filename, \"append\", str(train_result_at_epoch), screen=True)\n",
        "\n",
        "    # Write train_val_results to file\n",
        "    message = \"* Saving train, val results (all epochs): 'epoch, best_acc_train, best_loss_train, best_acc, best_loss'\"\n",
        "    #myprint(log_filename, \"append\", message, screen=True)\n",
        "    np.save(train_val_filename, train_val_results)\n",
        "\n",
        "    # Print final results\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    time_elapsed = time.time() - since\n",
        "    total_training_time = \"{:.0f}m {:.0f}s\".format(time_elapsed // 60, time_elapsed % 60)\n",
        "    message = \"  => Training completed in \" + total_training_time\n",
        "    total_training_time = \"{:.0f}m {:.0f}s\".format(time_elapsed // 60, time_elapsed % 60)\n",
        "    #myprint(log_filename, \"append\", message, screen=True)\n",
        "    message = \"  => Best test loss: {:.4f} | Best test accuracy: {:.4f}\".format(best_loss, best_acc)\n",
        "    #myprint(log_filename, \"append\", message, screen=True)\n",
        "    print(\n",
        "        \"Training completed in {:.0f}m {:.0f}s\".format(time_elapsed // 60, time_elapsed % 60)\n",
        "    )\n",
        "    print(\"Best test loss: {:.4f} | Best test accuracy: {:.4f}\".format(best_loss, best_acc))\n",
        "    return model, total_training_time, train_Acc, train_Loss, val_Acc, val_Loss, train_val_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XpNDj--HspMT"
      },
      "outputs": [],
      "source": [
        "model_hybrid, total_training_time, train_Acc, train_Loss, val_Acc, val_Loss, train_val_results = train_model(model_hybrid, criterion, optimizer_hybrid, exp_lr_scheduler, num_epochs=num_epochs, temp_model_name = interim_model_name)\n",
        "message = \"* (FINISH re-training the Quantum Layer).\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XJH6cFfufClR"
      },
      "outputs": [],
      "source": [
        "#myprint(log_filename, \"append\", message, screen=True)\n",
        "for x in train_val_results:\n",
        " print(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LWa_YB40VJs3"
      },
      "outputs": [],
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "running_loss = 0.0\n",
        "running_corrects = 0\n",
        "n_batches = dataset_sizes['validation'] // batch_size\n",
        "it = 0\n",
        "\n",
        "# Testing loop\n",
        "for inputs, labels in dataloaders['validation']:\n",
        "    model_hybrid.eval()\n",
        "    inputs = inputs.to(device)\n",
        "    labels = labels.to(device)\n",
        "    batch_size_ = len(inputs)\n",
        "    with torch.set_grad_enabled(False):\n",
        "     outputs = model_hybrid(inputs)\n",
        "     _, preds = torch.max(outputs, 1)\n",
        "     loss = criterion(outputs, labels)\n",
        "     running_loss += loss.item() * batch_size_\n",
        "     batch_corrects = torch.sum(preds == labels.data).item()\n",
        "     running_corrects += batch_corrects\n",
        "     print('Iter: {}/{}'.format(it + 1, n_batches + 1), end='\\r', flush=True)\n",
        "     it+=1\n",
        "# Print final results\n",
        "epoch_loss = running_loss / dataset_sizes['validation']\n",
        "epoch_acc = running_corrects / dataset_sizes['validation']\n",
        "print('\\nTest Loss: {:.4f} Test Acc: {:.4f}        '.format(epoch_loss, epoch_acc))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iJ4kghoAnPBV"
      },
      "outputs": [],
      "source": [
        "def visualize_model(model, num_images=6, fig_name=\"Predictions\"):\n",
        "    images_so_far = 0\n",
        "    _fig = plt.figure(fig_name)\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for _i, (inputs, labels) in enumerate(dataloaders[\"validation\"]):\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            for j in range(inputs.size()[0]):\n",
        "                images_so_far += 1\n",
        "                ax = plt.subplot(num_images // 2, 2, images_so_far)\n",
        "                ax.axis(\"off\")\n",
        "                ax.set_title(\"[{}]\".format(class_names[preds[j]]))\n",
        "                imshow(inputs.cpu().data[j])\n",
        "                if images_so_far == num_images:\n",
        "                    return"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "noN34B5VXghe"
      },
      "outputs": [],
      "source": [
        "visualize_model(model_hybrid, num_images=batch_size)\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1bv7Q9oq2YzLxurE8rO6sXLQGQMaBFdFl",
      "authorship_tag": "ABX9TyOqS4DIL759/0hB4Krs1MQi",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}